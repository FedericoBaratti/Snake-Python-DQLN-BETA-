# Snake-Python-DQLN

<div align="center">
  <br>
  <h3>ğŸ Advanced Deep Q-Learning Network for Snake Game ğŸ§ </h3>
  <br>
  <p>
    <a href="#overview">Overview</a> â€¢
    <a href="#features">Features</a> â€¢
    <a href="#installation">Installation</a> â€¢
    <a href="#usage">Usage</a> â€¢
    <a href="#results">Results</a> â€¢
    <a href="#changelog">Changelog</a>
  </p>
</div>

## Overview

**Snake-Python-DQLN** is a sophisticated implementation of the classic Snake game powered by state-of-the-art Deep Q-Learning techniques. This project demonstrates how modern reinforcement learning algorithms can master complex sequential decision-making tasks through a highly modular and extensible software architecture.

<div align="center">
  <table>
    <tr>
      <td align="center"><b>ğŸ§  Deep Learning</b></td>
      <td align="center"><b>ğŸ® Classic Gameplay</b></td>
      <td align="center"><b>ğŸ“Š Advanced Metrics</b></td>
    </tr>
    <tr>
      <td align="center">Cutting-edge neural networks</td>
      <td align="center">Nostalgic yet challenging</td>
      <td align="center">Comprehensive performance tracking</td>
    </tr>
  </table>
</div>

## Features

### ğŸš€ Advanced DQN Variants

- **Double DQN** - Eliminates value overestimation bias
- **Dueling DQN** - Separates state value and action advantage estimation
- **Noisy Networks** - Implements parameterized exploration strategies
- **Prioritized Experience Replay** - Focuses learning on informative transitions
- **N-step Returns** - Accelerates reward propagation through the network

### ğŸ› ï¸ Technical Components

<div align="center">
  <table>
    <tr>
      <td align="center" width="33%"><b>Game Engine</b></td>
      <td align="center" width="33%"><b>RL Framework</b></td>
      <td align="center" width="33%"><b>Visualization</b></td>
    </tr>
    <tr>
      <td>
        â€¢ Configurable grid size<br>
        â€¢ Customizable reward function<br>
        â€¢ Real-time collision detection
      </td>
      <td>
        â€¢ Tensor-based state representation<br>
        â€¢ GPU-accelerated training<br>
        â€¢ Checkpoint management system
      </td>
      <td>
        â€¢ Interactive gameplay<br>
        â€¢ Performance dashboards<br>
        â€¢ Learning progression graphs
      </td>
    </tr>
  </table>
</div>

## Installation

### Prerequisites

- Python 3.6+
- CUDA-compatible GPU (recommended)

### Quick Setup

```bash
# Clone repository
git clone https://github.com/yourusername/Snake-Python-DQLN.git
cd Snake-Python-DQLN

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

<details>
<summary><b>Detailed installation instructions</b></summary>

For GPU acceleration:
```bash
# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
```

For visualization tools:
```bash
pip install pygame matplotlib seaborn
```

See [docs/installation.md](docs/installation.md) for complete installation guide.
</details>

## Usage

### Training the Agent

```bash
python train.py --agent dqn --model dueling --buffer prioritized --double True --n_steps 3
```

<details>
<summary><b>Training parameters</b></summary>

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--agent` | Agent type (dqn) | dqn |
| `--model` | Network architecture (dqn, dueling, noisy, noisy_dueling) | dqn |
| `--buffer` | Experience buffer type (standard, prioritized) | standard |
| `--double` | Enable Double DQN | False |
| `--n_steps` | Number of steps for N-step returns | 1 |
| `--hidden_dim` | Hidden layer dimensions | "128,128" |
| `--lr` | Learning rate | 1e-4 |
| `--gamma` | Discount factor | 0.99 |
| `--total_episodes` | Total training episodes | 10000 |

Run `python train.py --help` for all options.
</details>

### Playing the Game

```bash
# Watch AI play
python play.py --mode ai --model_path results/best_model.pt --speed 15

# Play yourself
python play.py --mode human --grid_size 20
```

### Interactive Mode

```bash
# Launch interactive interface
python main.py
```


<summary><b>Project Structure</b></summary>

```
Snake-Python-DQLN/
â”œâ”€â”€ agents/                  # RL algorithm implementations
â”‚   â”œâ”€â”€ dqn/                 # Deep Q-Network implementation
â”‚   â”‚   â”œâ”€â”€ agent.py         # DQN agent logic
â”‚   â”‚   â”œâ”€â”€ buffer.py        # Experience memory buffer
â”‚   â”‚   â”œâ”€â”€ network.py       # Neural network definitions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ core/                    # Application core
â”‚   â”œâ”€â”€ environment.py       # Environmental wrapper for RL
â”‚   â”œâ”€â”€ snake_game.py        # Core game implementation
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                   # Utility components
â”‚   â”œâ”€â”€ common.py            # Common functions
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ hardware_utils.py    # Hardware optimization
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ config/                  # Configurations
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ results/                 # Experiment results
â”œâ”€â”€ main.py                  # Entry point
â”œâ”€â”€ train.py                 # Training pipeline
â”œâ”€â”€ play.py                  # UI and visualization
â””â”€â”€ requirements.txt         # Dependencies
```
</details>

For comprehensive architecture details, see [docs/architecture.md](docs/architecture.md).



### Performance Metrics

| Model | Avg. Score | Max Score | Training Time |
|-------|------------|-----------|---------------|
| DQN   | 32.5       | 78        | 2.5 hours     |
| Double DQN | 45.3  | 112       | 3.2 hours     |
| Dueling DQN | 62.7 | 165       | 3.8 hours     |
| Full (All Features) | 87.2 | 243 | 4.5 hours    |

## Practical Applications

Beyond the classic Snake game, this architecture demonstrates techniques applicable to:

- **Robotics Navigation** - Path planning and obstacle avoidance
- **Resource Management** - Optimizing sequential allocation decisions
- **Autonomous Systems** - Self-improving decision making in complex environments

## Changelog

### Version 2.0.1 (Current)
- **Architecture Redesign**: Complete restructuring of project architecture with cleaner separation of concerns
- **Enhanced DQN Implementation**: 
  - Added Dueling DQN architecture
  - Implemented Noisy Networks for improved exploration strategies
  - Added support for N-step Returns
  - Improved Double DQN implementation
- **Performance Optimization**: 
  - GPU acceleration for neural network training
  - Hardware-optimized tensor operations
  - Improved checkpoint management system
- **Documentation Improvements**:
  - Comprehensive architecture documentation
  - Detailed installation guides
  - Extended usage documentation
- **UI Enhancements**:
  - New interactive gameplay mode
  - Performance dashboards for real-time monitoring
  - Learning progression visualization
- **Core Refactoring**:
  - Redesigned snake game core with improved collision detection
  - Configurable environment parameters
  - More sophisticated reward function options
- **Code Quality**:
  - Improved modularity and extensibility
  - Better separation of concerns
  - More comprehensive test coverage
  - English localization of all documentation and UI elements

### Version 2.0.0
- Initial introduction of visual model selection interface
- Dynamic checkpoint loading system
- Improved UI with advanced visual feedback
- Added quick-start parameter `--select-model`
- Updated documentation and user manual

### Version 1.4.2
- Initial beta release with basic DQN implementation
- Simple game environment with fixed grid size
- Basic visualization capabilities
- Limited training configuration options
- Eng documentation and UI

### Version 1.3.7
- Added visual model selection interface
- Implemented dynamic checkpoint loading system
- Improved user interface with advanced visual feedback
- Added quick start parameter --select-model
- Updated documentation and user manual

### Version 1.0
- Initial release with basic features
- Implementation of Snake game with manual controls
- DQN agent with autoplay mode support
- Training system with checkpoints
- Graphical interface in Pygame

## Contributors

- Federico Baratti

## License

Released under the MIT License.


